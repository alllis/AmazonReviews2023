{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRJ9MvZibFJZ+GMGFJhaeo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alllis/AmazonReviews2023/blob/main/PositionManagement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0rYmSmVYwSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df68e76-eae8-4d4e-cc95-8e12811a4335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting shimmy>=2.0\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy>=2.0) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy>=2.0) (1.1.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting gymnasium<1.1.0,>=0.29.1 (from stable-baselines3)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.1.0\n",
            "    Uninstalling gymnasium-1.1.0:\n",
            "      Successfully uninstalled gymnasium-1.1.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed gymnasium-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install numpy\n",
        "!pip install 'shimmy>=2.0'\n",
        "!pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces"
      ],
      "metadata": {
        "id": "JP_JEofICH1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class MechanicalTradingEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(MechanicalTradingEnv, self).__init__()\n",
        "\n",
        "        # Fixed buy and sell levels\n",
        "        self.buy_levels = np.array([90, 80])  # Buy at these price levels\n",
        "        self.sell_levels = np.array([100, 105])  # Sell at these price levels\n",
        "\n",
        "        # Allocations (initial values, will be optimized by RL later)\n",
        "        self.buy_allocations = np.array([0.4, 0.6])  # Buy allocations: 40% at 90, 60% at 80\n",
        "        self.sell_allocations = np.array([0.4, 0.6])  # Sell allocations: 40% at 100, 60% at 105\n",
        "\n",
        "        # Position tracking\n",
        "        self.current_position = 0.0  # Total % of capital invested\n",
        "        self.last_buy_level = None  # Last buy level\n",
        "        self.last_sell_level = None  # Last sell level\n",
        "\n",
        "        # Observation space: [current_price, current_position]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0]),\n",
        "            high=np.array([np.inf, 1.0]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Action space (for RL optimization in the future)\n",
        "        self.action_space = spaces.Box(\n",
        "            low=0.0,\n",
        "            high=1.0,\n",
        "            shape=(len(self.buy_levels) + len(self.sell_levels),),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Initialize environment variables\n",
        "        self.current_price = 100.0  # Start at neutral price\n",
        "        self.remaining_capital = 1.0  # Total capital (normalized to 1)\n",
        "        self.total_profit = 0.0  # Track realized profit\n",
        "        self.time_step = 0\n",
        "        self.max_steps = 100\n",
        "        self.trades = []  # Record trades\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_price = 100.0\n",
        "        self.remaining_capital = 1.0\n",
        "        self.current_position = 0.0\n",
        "        self.total_profit = 0.0\n",
        "        self.last_buy_level = None\n",
        "        self.last_sell_level = None\n",
        "        self.time_step = 0\n",
        "        self.trades = []\n",
        "        state = np.array([self.current_price, self.current_position], dtype=np.float32)\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        # In future, RL will optimize these allocations\n",
        "        self.buy_allocations = action[:len(self.buy_levels)]\n",
        "        self.sell_allocations = action[len(self.buy_levels):]\n",
        "\n",
        "        reward = 0.0  # Track realized profit for this step\n",
        "\n",
        "        # **Handle Buy Actions**\n",
        "        for i, level in enumerate(self.buy_levels):\n",
        "            if self.current_price < level:\n",
        "                allocated_amount = self.buy_allocations[i]  # How much to buy at this level\n",
        "\n",
        "                if i == 0:  # Buy level 90\n",
        "                    if self.current_position < self.buy_allocations[0]:  # Only buy if we haven't hit 40%\n",
        "                        self.current_position += allocated_amount\n",
        "                        self.remaining_capital -= allocated_amount\n",
        "                        self.last_buy_level = level\n",
        "                        self.trades.append(f\"Buy {allocated_amount:.2f} at {self.current_price:.2f} (Level: {level})\")\n",
        "\n",
        "                elif i == 1:  # Buy level 80\n",
        "                    if self.current_position < sum(self.buy_allocations):  # Only buy if position < 100%\n",
        "                        additional_allocation = sum(self.buy_allocations) - self.current_position\n",
        "                        allocation = min(allocated_amount, additional_allocation)\n",
        "                        self.current_position += allocation\n",
        "                        self.remaining_capital -= allocation\n",
        "                        self.last_buy_level = level\n",
        "                        self.trades.append(f\"Buy {allocation:.2f} at {self.current_price:.2f} (Level: {level})\")\n",
        "\n",
        "        # **Handle Sell Actions**\n",
        "        for i, level in enumerate(self.sell_levels):\n",
        "            if self.current_price >= level and self.current_position > 0:\n",
        "                allocated_amount = self.sell_allocations[i] * self.current_position  # Sell % of current position\n",
        "\n",
        "                if i == 0:  # Sell level 100\n",
        "                    if self.last_sell_level != level:  # Ensure no duplicate sell at the same level\n",
        "                        self.current_position -= allocated_amount\n",
        "                        self.remaining_capital += allocated_amount * self.current_price\n",
        "                        self.total_profit += allocated_amount * (self.current_price - level)  # Profit calc\n",
        "                        self.last_sell_level = level\n",
        "                        self.trades.append(f\"Sell {allocated_amount:.2f} at {self.current_price:.2f} (Level: {level})\")\n",
        "\n",
        "                elif i == 1:  # Sell level 105\n",
        "                    self.current_position -= allocated_amount\n",
        "                    self.remaining_capital += allocated_amount * self.current_price\n",
        "                    self.total_profit += allocated_amount * (self.current_price - level)\n",
        "                    self.trades.append(f\"Sell {allocated_amount:.2f} at {self.current_price:.2f} (Level: {level})\")\n",
        "\n",
        "        # **Simulate Price Movement**\n",
        "        drift = 0.1\n",
        "        volatility = 1.5\n",
        "        self.current_price += np.random.normal(drift, volatility)\n",
        "\n",
        "        # Increment time step\n",
        "        self.time_step += 1\n",
        "        done = self.time_step >= self.max_steps\n",
        "\n",
        "        # Create new state\n",
        "        state = np.array([self.current_price, self.current_position], dtype=np.float32)\n",
        "\n",
        "        return state, self.total_profit, done, {}\n",
        "\n",
        "# Initialize the environment\n",
        "env = MechanicalTradingEnv()\n"
      ],
      "metadata": {
        "id": "_gIvlXbmO0gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "obs = env.reset()\n",
        "for _ in range(100):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    print(f\"Action: {action}, Reward: {reward}, Next State: {obs}\")\n",
        "    if done:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53pgqARPOUvv",
        "outputId": "c9b7de74-f2da-453d-a5ab-6a6caae5a029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 100      |\n",
            "|    ep_rew_mean     | 0        |\n",
            "| time/              |          |\n",
            "|    fps             | 1112     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 100          |\n",
            "|    ep_rew_mean          | 0.315        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 819          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 5            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041102143 |\n",
            "|    clip_fraction        | 0.0213       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.67        |\n",
            "|    explained_variance   | 0.000993     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0117       |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00107     |\n",
            "|    std                  | 0.997        |\n",
            "|    value_loss           | 0.0297       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 100         |\n",
            "|    ep_rew_mean          | 0.575       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 764         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006182332 |\n",
            "|    clip_fraction        | 0.0434      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.65       |\n",
            "|    explained_variance   | 0.0198      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00789     |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00254    |\n",
            "|    std                  | 0.991       |\n",
            "|    value_loss           | 0.505       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 100         |\n",
            "|    ep_rew_mean          | 0.433       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 701         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003293651 |\n",
            "|    clip_fraction        | 0.00488     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.63       |\n",
            "|    explained_variance   | 0.132       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.173       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.000441   |\n",
            "|    std                  | 0.989       |\n",
            "|    value_loss           | 0.793       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 100          |\n",
            "|    ep_rew_mean          | 0.711        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 704          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055651027 |\n",
            "|    clip_fraction        | 0.0426       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.62        |\n",
            "|    explained_variance   | 0.499        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0211       |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.0026      |\n",
            "|    std                  | 0.985        |\n",
            "|    value_loss           | 0.0357       |\n",
            "------------------------------------------\n",
            "Action: [0.         0.09025657 1.         0.        ], Reward: 0.0, Next State: [100.63467   0.     ]\n",
            "Action: [0.         0.06192947 0.         0.6575134 ], Reward: 0.0, Next State: [99.632835  0.      ]\n",
            "Action: [1.         0.34125334 0.5762534  0.5033382 ], Reward: 0.0, Next State: [98.8351  0.    ]\n",
            "Action: [0.17714971 0.         0.         0.        ], Reward: 0.0, Next State: [98.30852  0.     ]\n",
            "Action: [0.04998019 0.5465365  1.         0.        ], Reward: 0.0, Next State: [95.57588  0.     ]\n",
            "Action: [0.55278206 0.         0.         0.        ], Reward: 0.0, Next State: [98.50824  0.     ]\n",
            "Action: [1.         0.34470636 0.         1.        ], Reward: 0.0, Next State: [100.170166   0.      ]\n",
            "Action: [0.38071117 0.         0.         0.        ], Reward: 0.0, Next State: [98.7953  0.    ]\n",
            "Action: [0.3009022 0.6542765 0.        0.       ], Reward: 0.0, Next State: [97.8989  0.    ]\n",
            "Action: [0.48328996 0.         0.7979536  0.        ], Reward: 0.0, Next State: [98.91766  0.     ]\n",
            "Action: [0.        0.9338954 0.        0.1795561], Reward: 0.0, Next State: [98.011406  0.      ]\n",
            "Action: [0. 0. 0. 0.], Reward: 0.0, Next State: [94.91261  0.     ]\n",
            "Action: [0.39906952 0.19076452 0.         1.        ], Reward: 0.0, Next State: [92.682556  0.      ]\n",
            "Action: [0.         0.3395319  0.08253226 0.        ], Reward: 0.0, Next State: [94.44238  0.     ]\n",
            "Action: [0.         0.         0.47044593 0.        ], Reward: 0.0, Next State: [96.95136  0.     ]\n",
            "Action: [0.05039842 0.9286821  0.         0.46932158], Reward: 0.0, Next State: [98.32129  0.     ]\n",
            "Action: [0.5022497  0.7540928  0.99327415 0.28376788], Reward: 0.0, Next State: [97.16297  0.     ]\n",
            "Action: [1.        0.7387803 0.        0.       ], Reward: 0.0, Next State: [96.61409  0.     ]\n",
            "Action: [0.         0.         0.18115924 0.        ], Reward: 0.0, Next State: [95.801125  0.      ]\n",
            "Action: [0.42270762 0.         1.         0.28433228], Reward: 0.0, Next State: [95.62795  0.     ]\n",
            "Action: [0.        0.        0.        0.3719143], Reward: 0.0, Next State: [95.894424  0.      ]\n",
            "Action: [0.       0.855155 0.       1.      ], Reward: 0.0, Next State: [95.14025  0.     ]\n",
            "Action: [0.10710718 0.         0.4046777  0.2962184 ], Reward: 0.0, Next State: [93.09681  0.     ]\n",
            "Action: [0.         0.28093648 0.         0.        ], Reward: 0.0, Next State: [89.87299  0.     ]\n",
            "Action: [0. 1. 0. 0.], Reward: 0.0, Next State: [88.03171  0.     ]\n",
            "Action: [0.5494541 0.        0.        1.       ], Reward: 0.0, Next State: [86.51822    0.5494541]\n",
            "Action: [0.        0.        1.        0.9600505], Reward: 0.0, Next State: [85.53296    0.5494541]\n",
            "Action: [0.49218893 0.         0.         0.69287986], Reward: 0.0, Next State: [87.49944    0.5494541]\n",
            "Action: [0. 0. 0. 0.], Reward: 0.0, Next State: [86.96501    0.5494541]\n",
            "Action: [0.        0.3378774 0.        0.       ], Reward: 0.0, Next State: [87.64907    0.5494541]\n",
            "Action: [0.        0.2812339 1.        1.       ], Reward: 0.0, Next State: [87.28482    0.5494541]\n",
            "Action: [0.31712756 0.         0.         0.3824646 ], Reward: 0.0, Next State: [86.718445   0.5494541]\n",
            "Action: [0.         0.         0.         0.48413745], Reward: 0.0, Next State: [87.8714     0.5494541]\n",
            "Action: [1. 1. 1. 0.], Reward: 0.0, Next State: [89.64711    1.5494541]\n",
            "Action: [1.         0.40044242 0.16052471 0.09656788], Reward: 0.0, Next State: [89.4582     1.5494541]\n",
            "Action: [1.        0.8358331 0.        1.       ], Reward: 0.0, Next State: [89.379425   1.5494541]\n",
            "Action: [1. 1. 0. 0.], Reward: 0.0, Next State: [89.1693     1.5494541]\n",
            "Action: [0.01045522 0.20069745 1.         0.        ], Reward: 0.0, Next State: [89.78214    1.5494541]\n",
            "Action: [1. 1. 0. 1.], Reward: 0.0, Next State: [89.05627    1.5494541]\n",
            "Action: [0.         0.         0.         0.01225833], Reward: 0.0, Next State: [86.501854   1.5494541]\n",
            "Action: [0.         0.4563363  0.09007754 1.        ], Reward: 0.0, Next State: [89.76659    1.5494541]\n",
            "Action: [0.         0.         0.20569175 0.        ], Reward: 0.0, Next State: [89.13576    1.5494541]\n",
            "Action: [0. 0. 0. 1.], Reward: 0.0, Next State: [88.1513     1.5494541]\n",
            "Action: [1.         0.         0.39841357 0.39739916], Reward: 0.0, Next State: [87.64161    1.5494541]\n",
            "Action: [0.        0.        0.2440684 0.       ], Reward: 0.0, Next State: [84.36712    1.5494541]\n",
            "Action: [0.         0.39017868 0.         1.        ], Reward: 0.0, Next State: [88.470634   1.5494541]\n",
            "Action: [0.83073664 0.         0.5648478  0.        ], Reward: 0.0, Next State: [91.2813     1.5494541]\n",
            "Action: [0.        0.6881567 0.        0.       ], Reward: 0.0, Next State: [88.4942     1.5494541]\n",
            "Action: [1. 0. 0. 0.], Reward: 0.0, Next State: [87.32351    1.5494541]\n",
            "Action: [0.         0.         0.27185437 0.71768594], Reward: 0.0, Next State: [87.46764    1.5494541]\n",
            "Action: [0. 1. 1. 0.], Reward: 0.0, Next State: [89.41774    1.5494541]\n",
            "Action: [1.         0.23519485 0.         0.9168087 ], Reward: 0.0, Next State: [89.06444    1.5494541]\n",
            "Action: [0.         0.         0.         0.22946964], Reward: 0.0, Next State: [90.17545    1.5494541]\n",
            "Action: [0. 0. 1. 0.], Reward: 0.0, Next State: [90.5518     1.5494541]\n",
            "Action: [0.65786564 0.         0.10279755 0.15054643], Reward: 0.0, Next State: [91.728004   1.5494541]\n",
            "Action: [0.         1.         0.60344756 0.7015247 ], Reward: 0.0, Next State: [94.267265   1.5494541]\n",
            "Action: [0.4171061  0.59971935 0.         0.        ], Reward: 0.0, Next State: [93.960236   1.5494541]\n",
            "Action: [0.         1.         0.33501953 0.        ], Reward: 0.0, Next State: [95.749626   1.5494541]\n",
            "Action: [0.7905453 1.        0.        0.       ], Reward: 0.0, Next State: [96.58647    1.5494541]\n",
            "Action: [0.         0.40505454 0.39254045 0.        ], Reward: 0.0, Next State: [96.31887    1.5494541]\n",
            "Action: [0.        0.        0.9862732 0.       ], Reward: 0.0, Next State: [98.1106     1.5494541]\n",
            "Action: [0.        0.        0.4709562 0.       ], Reward: 0.0, Next State: [98.32619    1.5494541]\n",
            "Action: [0.78009474 0.33326593 0.76196796 1.        ], Reward: 0.0, Next State: [97.93444    1.5494541]\n",
            "Action: [0.        0.        0.7062386 1.       ], Reward: 0.0, Next State: [98.318855   1.5494541]\n",
            "Action: [1.         1.         0.06914487 0.        ], Reward: 0.0, Next State: [96.30737    1.5494541]\n",
            "Action: [0.         0.16101152 0.         0.98941714], Reward: 0.0, Next State: [94.89236    1.5494541]\n",
            "Action: [0.         0.         0.         0.20345369], Reward: 0.0, Next State: [93.76448    1.5494541]\n",
            "Action: [0.         0.21331987 0.8298437  0.        ], Reward: 0.0, Next State: [94.53262    1.5494541]\n",
            "Action: [0.         0.         0.         0.63545185], Reward: 0.0, Next State: [95.15048    1.5494541]\n",
            "Action: [0.         0.07514412 0.         0.        ], Reward: 0.0, Next State: [97.3372     1.5494541]\n",
            "Action: [0.        0.        0.        0.9969629], Reward: 0.0, Next State: [96.88603    1.5494541]\n",
            "Action: [0.         0.         0.         0.02238152], Reward: 0.0, Next State: [96.8745     1.5494541]\n",
            "Action: [0.         0.20854864 1.         0.02001882], Reward: 0.0, Next State: [95.951324   1.5494541]\n",
            "Action: [0.         0.99731946 0.         0.5927616 ], Reward: 0.0, Next State: [94.37317    1.5494541]\n",
            "Action: [0. 0. 1. 0.], Reward: 0.0, Next State: [93.574265   1.5494541]\n",
            "Action: [0. 0. 0. 0.], Reward: 0.0, Next State: [93.56992    1.5494541]\n",
            "Action: [0. 1. 1. 1.], Reward: 0.0, Next State: [92.79707    1.5494541]\n",
            "Action: [1.        0.7763108 0.        0.       ], Reward: 0.0, Next State: [93.10861    1.5494541]\n",
            "Action: [0.         0.94198906 0.         0.        ], Reward: 0.0, Next State: [95.14181    1.5494541]\n",
            "Action: [1.         0.57647383 0.         0.        ], Reward: 0.0, Next State: [96.07434    1.5494541]\n",
            "Action: [0.       0.299399 0.       0.      ], Reward: 0.0, Next State: [97.73216    1.5494541]\n",
            "Action: [1. 0. 1. 0.], Reward: 0.0, Next State: [99.075645   1.5494541]\n",
            "Action: [0.58284414 0.87758136 0.24334763 0.        ], Reward: 0.0, Next State: [97.460266   1.5494541]\n",
            "Action: [0. 1. 1. 1.], Reward: 0.0, Next State: [100.679085    1.5494541]\n",
            "Action: [0.         0.57148546 0.         0.21232998], Reward: 0.0, Next State: [100.15951     1.5494541]\n",
            "Action: [0.24017374 0.65046346 0.14892049 0.6361137 ], Reward: 0.0, Next State: [98.95452    1.5494541]\n",
            "Action: [0.         1.         0.24717392 0.6819063 ], Reward: 0.0, Next State: [95.20688    1.5494541]\n",
            "Action: [0.5048735  1.         0.69607073 0.        ], Reward: 0.0, Next State: [92.39914    1.5494541]\n",
            "Action: [0.63844186 1.         0.         0.        ], Reward: 0.0, Next State: [93.74176    1.5494541]\n",
            "Action: [0.         0.01867748 0.         0.8125093 ], Reward: 0.0, Next State: [92.381966   1.5494541]\n",
            "Action: [0.2977698  0.36039993 0.         1.        ], Reward: 0.0, Next State: [91.02916    1.5494541]\n",
            "Action: [0.31573218 0.         0.         0.        ], Reward: 0.0, Next State: [92.65862    1.5494541]\n",
            "Action: [1.         0.03079247 0.         0.3167823 ], Reward: 0.0, Next State: [91.44971    1.5494541]\n",
            "Action: [0.7571858  1.         0.         0.59923863], Reward: 0.0, Next State: [90.71003    1.5494541]\n",
            "Action: [0.         0.52762204 0.         0.        ], Reward: 0.0, Next State: [92.01888    1.5494541]\n",
            "Action: [0. 0. 0. 0.], Reward: 0.0, Next State: [92.78644    1.5494541]\n",
            "Action: [1.         0.         0.7968764  0.05968686], Reward: 0.0, Next State: [93.684296   1.5494541]\n",
            "Action: [0.05923977 0.         0.141817   0.        ], Reward: 0.0, Next State: [94.188866   1.5494541]\n",
            "Action: [0.49290907 0.82153034 0.75884867 0.        ], Reward: 0.0, Next State: [93.72008    1.5494541]\n",
            "Action: [0.6183302  1.         0.10629174 0.        ], Reward: 0.0, Next State: [91.83124    1.5494541]\n"
          ]
        }
      ]
    }
  ]
}